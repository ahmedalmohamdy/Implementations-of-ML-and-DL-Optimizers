{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0c8a76",
   "metadata": {},
   "source": [
    "# Coded By <span style=\"color: red;\">Ahmed Almohamdy</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77623092",
   "metadata": {},
   "source": [
    "# GD optimizer for single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_fun(X, y, n_iter=1000, lr=0.01,convergance_rate=0.000001,norm_value_check=0.0001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # We need to append theta First due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "\n",
    "        # Step two: Predict using initial theta\n",
    "        h = theta_0 + theta_1 * X\n",
    "\n",
    "        # Step three: Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "\n",
    "        # Step four: Calculate gradient for weights\n",
    "        theta_0_grad=sum((h-y))/m\n",
    "        theta_1_grad=sum((h-y)*X)/m\n",
    "\n",
    "        # Gradient vector norm\n",
    "        grad_vector = np.linalg.norm(np.array([theta_0_grad, theta_1_grad]))\n",
    "\n",
    "        # Convergence check\n",
    "        if (grad_vector < norm_value_check) or ((i != 0) and (abs(losses[i - 1] - J) < convergance_rate)):\n",
    "            break\n",
    "\n",
    "        # Step Five: Update our weights\n",
    "        theta_0 = theta_0 - lr * theta_0_grad\n",
    "        theta_1 = theta_1 - lr * theta_1_grad\n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{theta_0_grad}]\\n [{theta_1_grad}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_vector}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "        \n",
    "    # Training report\n",
    "    print(\"****************** Training Report ********************\\n\")\n",
    "    print(f\"Gradient Descent converged after {i} iterations\\n\")\n",
    "    print(f\"theta_0_Opt: {theta_0}\\ntheta_1_Opt: {theta_1}\\n\")\n",
    "\n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "    # Report\n",
    "    print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "    print(f\"Cost = {final_cost}\\n\")\n",
    "    print(\"h(x) = y_predict:\")\n",
    "    print(final_predictions)\n",
    "    print(\"\\ny_actual:\")\n",
    "    print(y)\n",
    "    return losses, theta_0_all, theta_1_all,final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9860410",
   "metadata": {},
   "source": [
    "# GD optimizer for Multi variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acd7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one: Initialize parameters (theta_0 & theta_1)\n",
    "n_iter=10000\n",
    "thetas=np.zeros((4,1))\n",
    "lr=0.0001\n",
    "m=len(y2)\n",
    "\n",
    "# To save losses and theta\n",
    "losses=[]\n",
    "all_thetaas=[]\n",
    "\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # We need to append theta First due to initilaization \n",
    "    all_thetaas.append(thetas)\n",
    "\n",
    "    # Step two: predicate on initila theta\n",
    "    h=X2@thetas\n",
    "    h=h.reshape(-1,1)\n",
    "\n",
    "    # Step three: calculate J & Error Vector \n",
    "    error = h - y2\n",
    "    J = np.sum(error**2) / (2 * m)\n",
    "    losses.append(J)\n",
    "    \n",
    "    # Step four: calculate gradient for weights\n",
    "    grad_vector=(X2.T@error)/m #( n X m ), (m,1) \n",
    "    \n",
    "    # Gradient vector norm\n",
    "    grad_vector_norm = np.linalg.norm(grad_vector)\n",
    "    \n",
    "     # Convergence check\n",
    "    if (grad_vector_norm < 0.2) or ((i != 0) and (abs(losses[i - 1] - J) < 0.000001)):\n",
    "        break\n",
    "    \n",
    "    # Step Five: Update weights\n",
    "    thetas=thetas-lr*grad_vector\n",
    "    \n",
    "     # Print\n",
    "    print(f\"****************** Iteration {i} ********************\")\n",
    "    print(f\"\\nh(x):\\n{h}\\n\")\n",
    "    print(f\"Error Vector:\\n{error}\\n\")\n",
    "    print(f\"J = {J}\\n\")\n",
    "    print(f\"Gradient Vector:\\n{grad_vector}\\n\")\n",
    "    print(f\"Gradient Vector Norm: {grad_vector_norm}\\n\")\n",
    "    print(f\"thetas_new :\\n{thetas}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795354a",
   "metadata": {},
   "source": [
    "# Stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c74e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def St_GD_LR_1var(X, y, lr=0.01,n_iter=1000,convergance_rate=0.001,norm_value_check=0.001):\n",
    "    np.random.seed(101)\n",
    "    alldata = np.column_stack((X, y))\n",
    "    alldata = shuffle(alldata)\n",
    "    X = alldata[:, :-1].flatten()\n",
    "    y = alldata[:, -1]    \n",
    "    m = len(X)\n",
    "    \n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    for i in range(n_iter):\n",
    "        for j in range(m):\n",
    "            # We need to append theta First due to initialization\n",
    "            theta_0_all.append(theta_0)\n",
    "            theta_1_all.append(theta_1)\n",
    "\n",
    "            # Step two: Predict using initial theta\n",
    "            h = theta_0 + theta_1 * X[j]\n",
    "\n",
    "            # Step three: Calculate J & Error Vector\n",
    "            error_vector = h - y[j]\n",
    "            #J = np.sum(error_vector**2) / (2 * m)\n",
    "            J=(h-y[j])**2/2\n",
    "            losses.append(J)\n",
    "\n",
    "            # Step four: Calculate gradient for weights\n",
    "            theta_0_grad=(h-y[j])\n",
    "            theta_1_grad=(h-y[j])*X[j]\n",
    "            \n",
    "            # Gradient vector norm\n",
    "            grad_vector = np.linalg.norm(np.array([theta_0_grad, theta_1_grad]))\n",
    "            \n",
    "            if (grad_vector < norm_value_check):\n",
    "                return theta_0, theta_1, losses, theta_0_all, theta_1_all\n",
    "            \n",
    "            # Step Five: Update our weights\n",
    "            theta_0 = theta_0 - lr * theta_0_grad\n",
    "            theta_1 = theta_1 - lr * theta_1_grad\n",
    "\n",
    "\n",
    "        # Convergence check\n",
    "        if ((i > 1) and (abs(losses[- 1] - losses[-(m+1)]) < convergance_rate)):\n",
    "            break\n",
    "\n",
    "        # information each epoch\n",
    "        print(f\"****************** Epoch {i} ********************\")\n",
    "        print(f\"Cost = {J}\\n\")\n",
    "        print(\"Gradient Vector:\")\n",
    "        print(np.array([[theta_0_grad], [theta_1_grad]]))\n",
    "        print(f\"\\nGradient Vector Norm: {grad_vector}\\n\")\n",
    "        print(f\"theta_0_new : {theta_0}\")\n",
    "        print(f\"theta_1_new : {theta_1}\\n\")\n",
    "\n",
    "        \n",
    "    return theta_0,theta_1,losses, theta_0_all, theta_1_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac840c",
   "metadata": {},
   "source": [
    "# Mini_batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556560c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(X, y, lr=0.001, n_iter=1000, batch_size=32, convergance_rate=0.1, norm_value_check=0.1):\n",
    "    np.random.seed(101)\n",
    "    alldata = np.column_stack((X, y))\n",
    "    alldata = shuffle(alldata)\n",
    "    X = alldata[:, :-1].flatten()\n",
    "    y = alldata[:, -1]\n",
    "    \n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    number_of_batches = m // batch_size\n",
    "    \n",
    "    for epoch in range(n_iter):\n",
    "        for j in range(number_of_batches):\n",
    "            start = j * batch_size\n",
    "            end = start + batch_size\n",
    "\n",
    "            # We need to append theta First due to initialization\n",
    "            theta_0_all.append(theta_0)\n",
    "            theta_1_all.append(theta_1)\n",
    "\n",
    "            # Step two: Predict using initial theta\n",
    "            h = theta_0 + theta_1 * X[start:end]\n",
    "\n",
    "            # Step three: Calculate J & Error Vector\n",
    "            error_vector = h - y[start:end]\n",
    "            J = np.sum(error_vector**2) / (2 * batch_size)\n",
    "            losses.append(J)\n",
    "\n",
    "            # Step four: Calculate gradient for weights\n",
    "            theta_0_grad = np.sum(error_vector) / batch_size\n",
    "            theta_1_grad = np.sum(error_vector * X[start:end]) / batch_size\n",
    "\n",
    "            # Gradient vector norm\n",
    "            grad_vector = np.linalg.norm(np.array([theta_0_grad, theta_1_grad]))\n",
    "            # Convergence check\n",
    "            if (grad_vector < norm_value_check):\n",
    "                return theta_0, theta_1, losses, theta_0_all, theta_1_all\n",
    "            \n",
    "            # Step Five: Update our weights\n",
    "            theta_0 = theta_0 - lr * theta_0_grad\n",
    "            theta_1 = theta_1 - lr * theta_1_grad\n",
    "            \n",
    "\n",
    "        if ((epoch > 1) and (abs(losses[-1] - losses[-int(np.ceil(m / batch_size+1))]) < convergance_rate)):\n",
    "            break\n",
    "\n",
    "        # Information for each epoch\n",
    "        print(f\"****************** Epoch {epoch} ********************\")\n",
    "        print(f\"Cost = {J}\\n\")\n",
    "        print(\"Gradient Vector:\")\n",
    "        print(np.array([[theta_0_grad], [theta_1_grad]]))\n",
    "        print(f\"\\nGradient Vector Norm: {np.linalg.norm(np.array([theta_0_grad, theta_1_grad]))}\\n\")\n",
    "        print(f\"theta_0_new : {theta_0}\")\n",
    "        print(f\"theta_1_new : {theta_1}\\n\")\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccc5cf",
   "metadata": {},
   "source": [
    "# Momentum GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def momentum_gradient_descent(X, y, n_iter=1000, lr=0.01, gamma=0.9, convergance_rate=0.001, norm_value_check=0.001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    \n",
    "    momentum_theta_0 = 0\n",
    "    momentum_theta_1 = 0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Append theta first due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "\n",
    "        # Predict using initial theta\n",
    "        h = theta_0 + theta_1 * X\n",
    "\n",
    "        # Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "\n",
    "        # Calculate gradient for weights\n",
    "        theta_0_grad = sum((h - y)) / m\n",
    "        theta_1_grad = sum((h - y) * X) / m\n",
    "\n",
    "        # Gradient vector norm\n",
    "        grad_vector = np.linalg.norm(np.array([theta_0_grad, theta_1_grad]))\n",
    "\n",
    "        # Convergence check\n",
    "        if (grad_vector < norm_value_check) or ((i != 0) and (abs(losses[i - 1] - J) < convergance_rate)):\n",
    "            break\n",
    "\n",
    "        # Update momentum \n",
    "        momentum_theta_0 = gamma * momentum_theta_0 + lr * theta_0_grad\n",
    "        momentum_theta_1 = gamma * momentum_theta_1 + lr * theta_1_grad\n",
    "\n",
    "        # Update weights with momentum\n",
    "        theta_0 = theta_0 - momentum_theta_0\n",
    "        theta_1 = theta_1 - momentum_theta_1\n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{theta_0_grad}]\\n [{theta_1_grad}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_vector}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "\n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all, final_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869281f2",
   "metadata": {},
   "source": [
    "# Nag GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b63467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nag_gradient_descent(X, y, n_iter=300, lr=0.01, gamma=0.9, convergance_rate=0.001, norm_value_check=0.001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    v_0 = 0\n",
    "    v_1 = 0\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Append theta first due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "        \n",
    "        # Predict==> updated theta_temp\n",
    "        h = theta_0 + theta_1 * X \n",
    "        \n",
    "        # Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "        \n",
    "        # Theta 0 gradient & Theta 1 gradient\n",
    "        d_theta_0 = (1 / m) * np.sum(error_vector)      \n",
    "        d_theta_1 = (1 / m) * np.sum(error_vector * X)  \n",
    "        \n",
    "        grad_vec = np.array([d_theta_0, d_theta_1])\n",
    "        grad_norm = np.linalg.norm(grad_vec)\n",
    "        \n",
    "        # Convergence check\n",
    "        if (grad_norm < norm_value_check) or ((i > 1) and (abs(losses[-2] - J) < convergance_rate)):\n",
    "            break\n",
    "        \n",
    "        theta_tmp_0  = theta_0 - gamma * v_0\n",
    "        theta_tmp_1  = theta_1 - gamma * v_1\n",
    "\n",
    "        h_temp = theta_tmp_0 + theta_tmp_1 * X\n",
    "\n",
    "        d_theta_tmp_0 = np.sum(h_temp - y) / m\n",
    "        d_theta_tmp_1 = (np.sum((h_temp - y) * X))/ m\n",
    "\n",
    "        v_0 = gamma * v_0 + lr * d_theta_tmp_0\n",
    "        v_1 = gamma * v_1 + lr * d_theta_tmp_1\n",
    "\n",
    "        theta_0 = theta_tmp_0 - lr * d_theta_tmp_0        \n",
    "        theta_1 = theta_tmp_1 - lr * d_theta_tmp_1 \n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{d_theta_0}]\\n [{d_theta_1}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_vec}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "    \n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "    print(f\"Results for gamma = {gamma}:\")\n",
    "    print(f\"Final theta_0: {theta_0}\")\n",
    "    print(f\"Final theta_1: {theta_1}\")\n",
    "    print(f\"Final Cost: {final_cost}\")\n",
    "    print(f\"Final Predictions: {final_predictions}\\n\")\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all, final_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e37db",
   "metadata": {},
   "source": [
    "# Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad( X, y, alpha,epsilon, iterations=1000, theta_0=0, theta_1=0,convergance_rate=0.001, norm_value_check=0.001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    vt_0 = 0\n",
    "    vt_1 = 0\n",
    "    h=[]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Append theta first due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "\n",
    "        # Predict==> updated theta_temp\n",
    "        h = theta_0 + theta_1 * X\n",
    "\n",
    "        # Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "\n",
    "        # Theta 0 gradient & Theta 1 gradient\n",
    "        d_theta_0 = (1 / m) * np.sum(error_vector)\n",
    "        d_theta_1 = (1 / m) * np.sum(error_vector * X)\n",
    "\n",
    "        grad_vec = np.array([d_theta_0, d_theta_1])\n",
    "        grad_norm = np.linalg.norm(grad_vec)\n",
    "\n",
    "        # Convergence check\n",
    "        if (grad_norm < norm_value_check) or ((i > 1) and (abs(losses[-2] - J) < convergance_rate)):\n",
    "            return theta_0, theta_1, losses, theta_0_all, theta_1_all,h\n",
    "\n",
    "        vt_0 = vt_0+(d_theta_0**2)\n",
    "        vt_1 = vt_1+(d_theta_1**2)\n",
    "\n",
    "         #update theta\n",
    "        theta_0=theta_0 - (alpha / np.sqrt(vt_0)+epsilon)*d_theta_0\n",
    "        theta_1=theta_1 - (alpha / np.sqrt(vt_1)+epsilon)*d_theta_1\n",
    "\n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{d_theta_0}]\\n [{d_theta_1}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_norm}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "\n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "    print(f\"Final theta_0: {theta_0}\")\n",
    "    print(f\"Final theta_1: {theta_1}\")\n",
    "    print(f\"Final Cost: {final_cost}\")\n",
    "    print(f\"Final Predictions: {final_predictions}\\n\")\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all,h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007edf78",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ad678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSProp( X, y, alpha,beta,epsilon, iterations=1000, theta_0=0, theta_1=0,convergance_rate=0.001, norm_value_check=0.001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    vt_0 = 0\n",
    "    vt_1 = 0\n",
    "    h=[]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Append theta first due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "\n",
    "        # Predict==> updated theta_temp\n",
    "        h = theta_0 + theta_1 * X\n",
    "\n",
    "        # Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "\n",
    "        # Theta 0 gradient & Theta 1 gradient\n",
    "        d_theta_0 = (1 / m) * np.sum(error_vector)\n",
    "        d_theta_1 = (1 / m) * np.sum(error_vector * X)\n",
    "\n",
    "        grad_vec = np.array([d_theta_0, d_theta_1])\n",
    "        grad_norm = np.linalg.norm(grad_vec)\n",
    "\n",
    "        # Convergence check\n",
    "        if (grad_norm < norm_value_check) or ((i > 1) and (abs(losses[-2] - J) < convergance_rate)):\n",
    "            return theta_0, theta_1, losses, theta_0_all, theta_1_all,h\n",
    "\n",
    "        vt_0 = (beta*vt_0)+((1-beta)*(d_theta_0**2))\n",
    "        vt_1 = beta*vt_1+((1-beta)*(d_theta_1**2))\n",
    "\n",
    "         #update theta\n",
    "        theta_0=theta_0 - (alpha / np.sqrt(vt_0)+epsilon)*d_theta_0\n",
    "        theta_1=theta_1 - (alpha / np.sqrt(vt_1)+epsilon)*d_theta_1\n",
    "\n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{d_theta_0}]\\n [{d_theta_1}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_norm}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "\n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "    print(f\"Final theta_0: {theta_0}\")\n",
    "    print(f\"Final theta_1: {theta_1}\")\n",
    "    print(f\"Final Cost: {final_cost}\")\n",
    "    print(f\"Final Predictions: {final_predictions}\\n\")\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all,h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e42a7",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(X, y, alpha, beta1,beta2, epsilon, iterations=1000, theta_0=0, theta_1=0,convergance_rate=0.001, norm_value_check=0.001):\n",
    "    m = len(X)\n",
    "    theta_0 = 0\n",
    "    theta_1 = 0\n",
    "    losses = []\n",
    "    theta_0_all = []\n",
    "    theta_1_all = []\n",
    "    vt_0 = 0\n",
    "    vt_1 = 0\n",
    "    mt_theta_0=0\n",
    "    mt_theta_1=1\n",
    "    h=[]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Append theta first due to initialization\n",
    "        theta_0_all.append(theta_0)\n",
    "        theta_1_all.append(theta_1)\n",
    "\n",
    "        # Predict==> updated theta_temp\n",
    "        h = theta_0 + theta_1 * X\n",
    "\n",
    "        # Calculate J & Error Vector\n",
    "        error_vector = h - y\n",
    "        J = np.sum(error_vector**2) / (2 * m)\n",
    "        losses.append(J)\n",
    "\n",
    "        # Theta 0 gradient & Theta 1 gradient\n",
    "        d_theta_0 = (1 / m) * np.sum(error_vector)\n",
    "        d_theta_1 = (1 / m) * np.sum(error_vector * X)\n",
    "\n",
    "        grad_vec = np.array([d_theta_0, d_theta_1])\n",
    "        grad_norm = np.linalg.norm(grad_vec)\n",
    "\n",
    "        # Convergence check\n",
    "        if (grad_norm < norm_value_check) or ((i > 1) and (abs(losses[-2] - J) < convergance_rate)):\n",
    "            return theta_0, theta_1, losses, theta_0_all, theta_1_all,h\n",
    "\n",
    "        # momentum equationS\n",
    "        mt_theta_0 = (mt_theta_0 * beta1) + ((1 - beta1) * d_theta_0)\n",
    "        mt_theta_1 = (mt_theta_1 * beta1) + ((1 - beta1) * d_theta_1)\n",
    "\n",
    "        # rms equations\n",
    "        # calculate vt_1\n",
    "        vt_0 = (beta2 * vt_0) + ((1 - beta2) * (d_theta_0 ** 2))\n",
    "        vt_1 = (beta2 * vt_1) + ((1 - beta2) * (d_theta_1 ** 2))\n",
    "\n",
    "        # Bias correction\n",
    "        mt_theta0_h = mt_theta_0 / (1 - pow(beta1,i+1))\n",
    "        mt_theta1_h = mt_theta_1 / (1 - pow(beta1,i+1))\n",
    "\n",
    "        vt_theta0_h = vt_0 / (1 - pow(beta2,i+1))\n",
    "        vt_theta1_h = vt_1 / (1 - pow(beta2,i+1))\n",
    "\n",
    "\n",
    "\n",
    "        # adam update theta\n",
    "        theta_0 = theta_0 - (alpha / (np.sqrt(vt_theta0_h) + epsilon) * mt_theta0_h)\n",
    "        theta_1 = theta_1 - (alpha / (np.sqrt(vt_theta1_h) + epsilon) * mt_theta1_h)\n",
    "\n",
    "\n",
    "        # Print like verbose we have in sklearn\n",
    "        print(f\"****************** Iteration {i} ********************\")\n",
    "        print(f\"\\nh(x): {h}\\n\")\n",
    "        print(f\"Error Vector:\\n{error_vector}\\n\")\n",
    "        print(f\"J = {J}\\n\")\n",
    "        print(f\"Gradient Vector:\\n[[{d_theta_0}]\\n [{d_theta_1}]]\\n\")\n",
    "        print(f\"Gradient Vector Norm: {grad_norm}\\n\")\n",
    "        print(f\"theta_0_new: {theta_0}\\ntheta_1_new: {theta_1}\\n\")\n",
    "\n",
    "    # Final h with error_vector\n",
    "    error_vector = (theta_0 + theta_1 * X) - y\n",
    "    # Final Cost\n",
    "    final_cost = np.sum(error_vector**2) / (2 * m)\n",
    "    # predictions\n",
    "    final_predictions = theta_0 + theta_1 * X\n",
    "    print(f\"Final theta_0: {theta_0}\")\n",
    "    print(f\"Final theta_1: {theta_1}\")\n",
    "    print(f\"Final Cost: {final_cost}\")\n",
    "    print(f\"Final Predictions: {final_predictions}\\n\")\n",
    "\n",
    "    return theta_0, theta_1, losses, theta_0_all, theta_1_all,h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
